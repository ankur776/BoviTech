# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SzmZsBF3nf0UoiFFJQoAEEZ1DjmyR_nK
"""

#!/bin/bash
!curl -L -o ~/cows-and-buffalo-computer-vision-dataset.zip\
  https://www.kaggle.com/api/v1/datasets/download/raghavdharwal/cows-and-buffalo-computer-vision-dataset
!unzip ~/cows-and-buffalo-computer-vision-dataset.zip -d ~/cows-and-buffalo-computer-vision-dataset



"""## Train the model

### Subtask:
Train the model using the training data generator and validate it using the validation data generator.

**Reasoning**:
Train the model using the training data generator and validate it using the validation data generator as specified in the subtask.
"""

# Train the model
history = model.fit(
    train_generator,
    epochs=epochs,
    validation_data=val_generator
)

print("Model training finished.")

"""## Set up training parameters

### Subtask:
Define the optimizer, loss function, and metrics, as well as the number of epochs.

**Reasoning**:
Define the training parameters such as optimizer, loss function, metrics, and epochs as specified in the subtask.
"""

from tensorflow.keras.optimizers import Adam

# Define training parameters
optimizer = Adam(learning_rate=0.0001)
loss_function = 'categorical_crossentropy'
metrics = ['accuracy']
epochs = 10  # You can adjust the number of epochs

print("Training parameters set up.")

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define data generators with data augmentation for training and validation
train_datagen = ImageDataGenerator(
    rescale=1./255, # Normalize pixel values
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

val_datagen = ImageDataGenerator(rescale=1./255) # Only normalize for validation

# No augmentation for test data, just normalization
test_datagen = ImageDataGenerator(rescale=1./255)

# Specify the directories for each split
train_dir = os.path.join(base_split_dir, 'train')
val_dir = os.path.join(base_split_dir, 'val')
test_dir = os.path.join(base_split_dir, 'test')

# Create data generators
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=target_size, # Use the target_size defined earlier
    batch_size=32,
    class_mode='categorical' # Use 'categorical' for one-hot encoding of labels
)

val_generator = val_datagen.flow_from_directory(
    val_dir,
    target_size=target_size,
    batch_size=32,
    class_mode='categorical'
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=target_size,
    batch_size=32,
    class_mode='categorical',
    shuffle=False # Keep data in order for evaluation
)

print("Data generators set up.")

import os
import cv2
import numpy as np

image_files = [f for f in os.listdir(image_directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

for file_name in image_files:
    file_path = os.path.join(image_directory, file_name)
    img = cv2.imread(file_path)
    if img is not None:
        # Convert to float32 for normalization
        img = img.astype(np.float32)
        # Normalize pixel values to [0, 1]
        normalized_img = img / 255.0
        # Convert back to uint8 for saving
        normalized_img = (normalized_img * 255).astype(np.uint8)
        cv2.imwrite(file_path, normalized_img)
        print(f"Normalized and saved: {file_name}")
    else:
        print(f"Could not read image: {file_name}")

import random
import os
import cv2
import numpy as np

image_files = [f for f in os.listdir(image_directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

if len(image_files) > 0:
    sample_files = random.sample(image_files, min(5, len(image_files)))

    for file_name in sample_files:
        file_path = os.path.join(image_directory, file_name)
        img = cv2.imread(file_path)
        if img is not None:
            # Convert to float32 to check the range accurately
            img = img.astype(np.float32) / 255.0
            min_val = np.min(img)
            max_val = np.max(img)
            print(f"Pixel value range of {file_name}: [{min_val}, {max_val}]")
        else:
            print(f"Could not read image: {file_name}")
else:
    print("No image files found in the directory.")

"""# Task
Resize all images in the directory "./images" to 224x224 pixels.

## Install necessary libraries

### Subtask:
Install OpenCV for image processing.

**Reasoning**:
Install the `opencv-python` library using pip.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install opencv-python

"""## Define image directory and target size

### Subtask:
Specify the directory containing the images and the desired dimensions (e.g., 224x224).

**Reasoning**:
Specify the image directory and target size as requested by the subtask.
"""

image_directory = '/root/cows-and-buffalo-computer-vision-dataset/train/images'
target_size = (224, 224)

"""## Resize images

### Subtask:
Iterate through the image files, read each image using OpenCV, resize it to the target size, and save it back to disk, overwriting the original image.

**Reasoning**:
Iterate through the image files, read each image using OpenCV, resize it to the target size, and save it back to disk.
"""

import cv2
import os

image_files = os.listdir(image_directory)

for file_name in image_files:
    file_path = os.path.join(image_directory, file_name)
    if os.path.isfile(file_path) and file_name.lower().endswith(('.png', '.jpg', '.jpeg')):
        img = cv2.imread(file_path)
        if img is not None:
            resized_img = cv2.resize(img, target_size)
            cv2.imwrite(file_path, resized_img)
            print(f"Resized and saved: {file_name}")

"""## Verify image dimensions

### Subtask:
(Optional) Check the dimensions of a few resized images to ensure the resizing was successful.

**Reasoning**:
Select a few image files, read them using OpenCV, get their dimensions, and print the dimensions to verify the resizing.
"""

import random
import os
import cv2

image_files = [f for f in os.listdir(image_directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

if len(image_files) > 0:
    sample_files = random.sample(image_files, min(5, len(image_files)))

    for file_name in sample_files:
        file_path = os.path.join(image_directory, file_name)
        img = cv2.imread(file_path)
        if img is not None:
            height, width, _ = img.shape
            print(f"Dimensions of {file_name}: ({width}, {height})")
        else:
            print(f"Could not read image: {file_name}")
else:
    print("No image files found in the directory.")

"""## Summary:

### Data Analysis Key Findings

*   The `opencv-python` library, necessary for image processing, was already installed.
*   The script successfully identified and processed image files with extensions '.png', '.jpg', and '.jpeg' within the specified directory.
*   All processed images were resized to the target dimensions of 224x224 pixels.
*   Verification of a sample of resized images confirmed that their dimensions are indeed 224x224.

### Insights or Next Steps

*   The process successfully resized all images in the specified directory to the desired dimensions, preparing them for potential use in a computer vision task.
*   Consider implementing error handling for cases where an image file might be corrupted or unreadable during the resizing process.

# Task
Split the dataset into training, validation, and testing sets with a 70/15/15 ratio.

## Define dataset split ratios

### Subtask:
Specify the desired percentage for the training, validation, and testing sets.

**Reasoning**:
Define the ratios for the training, validation, and testing sets as specified in the instructions.
"""

train_ratio = 0.70
val_ratio = 0.15
test_ratio = 0.15

"""## Identify image and label files

### Subtask:
Get the list of image files and their corresponding label files from the dataset directory.

**Reasoning**:
Get the list of image files and their corresponding label files from the dataset directory.
"""

label_directory = '/root/cows-and-buffalo-computer-vision-dataset/train/labels'

image_files = [f for f in os.listdir(image_directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

label_files = []
for image_file in image_files:
    base_name = os.path.splitext(image_file)[0]
    label_file_name = base_name + '.txt'
    label_file_path = os.path.join(label_directory, label_file_name)
    if os.path.exists(label_file_path):
        label_files.append(label_file_path)
    else:
        print(f"Warning: Label file not found for image: {image_file}")

print(f"Number of image files found: {len(image_files)}")
print(f"Number of label files found: {len(label_files)}")

"""## Split data

### Subtask:
Divide the image and label file paths into training, validation, and testing sets based on the defined ratios.

**Reasoning**:
Combine and shuffle the image and label file paths, then split them into training, validation, and testing sets based on the defined ratios.
"""

import random

# Combine image and label file paths
file_paths = []
for image_file in image_files:
    base_name = os.path.splitext(image_file)[0]
    label_file_name = base_name + '.txt'
    label_file_path = os.path.join(label_directory, label_file_name)
    image_file_path = os.path.join(image_directory, image_file)
    if os.path.exists(label_file_path):
        file_paths.append((image_file_path, label_file_path))
    else:
        print(f"Warning: Label file not found for image: {image_file}")

# Shuffle the combined list
random.shuffle(file_paths)

# Calculate the number of files for each split
total_files = len(file_paths)
train_count = int(total_files * train_ratio)
val_count = int(total_files * val_ratio)
test_count = total_files - train_count - val_count  # Ensure all files are included

# Split the shuffled list
train_files = file_paths[:train_count]
val_files = file_paths[train_count:train_count + val_count]
test_files = file_paths[train_count + val_count:]

print(f"Total file pairs: {total_files}")
print(f"Training set size: {len(train_files)}")
print(f"Validation set size: {len(val_files)}")
print(f"Testing set size: {len(test_files)}")

"""## Create directories for split datasets

### Subtask:
Create new directories to store the images and labels for each set (train, val, test).

**Reasoning**:
Define the base directory and create the necessary train, validation, and test directories for images and labels.
"""

import os

base_split_dir = '/root/cows-and-buffalo-computer-vision-dataset/split_data'

# Create base split directory if it doesn't exist
os.makedirs(base_split_dir, exist_ok=True)

# Create directories for training set images and labels
os.makedirs(os.path.join(base_split_dir, 'train', 'images'), exist_ok=True)
os.makedirs(os.path.join(base_split_dir, 'train', 'labels'), exist_ok=True)

# Create directories for validation set images and labels
os.makedirs(os.path.join(base_split_dir, 'val', 'images'), exist_ok=True)
os.makedirs(os.path.join(base_split_dir, 'val', 'labels'), exist_ok=True)

# Create directories for testing set images and labels
os.makedirs(os.path.join(base_split_dir, 'test', 'images'), exist_ok=True)
os.makedirs(os.path.join(base_split_dir, 'test', 'labels'), exist_ok=True)

print(f"Created directories in: {base_split_dir}")

"""## Copy files to respective directories

### Subtask:
Copy the images and label files to their corresponding train, validation, and test directories.

**Reasoning**:
Copy the image and label files to their corresponding train, validation, and test directories.
"""

import shutil

# Define destination directories
train_images_dir = os.path.join(base_split_dir, 'train', 'images')
train_labels_dir = os.path.join(base_split_dir, 'train', 'labels')
val_images_dir = os.path.join(base_split_dir, 'val', 'images')
val_labels_dir = os.path.join(base_split_dir, 'val', 'labels')
test_images_dir = os.path.join(base_split_dir, 'test', 'images')
test_labels_dir = os.path.join(base_split_dir, 'test', 'labels')

# Copy train files
for image_path, label_path in train_files:
    shutil.copy(image_path, os.path.join(train_images_dir, os.path.basename(image_path)))
    shutil.copy(label_path, os.path.join(train_labels_dir, os.path.basename(label_path)))
print(f"Copied {len(train_files)} files to training directories.")

# Copy validation files
for image_path, label_path in val_files:
    shutil.copy(image_path, os.path.join(val_images_dir, os.path.basename(image_path)))
    shutil.copy(label_path, os.path.join(val_labels_dir, os.path.basename(label_path)))
print(f"Copied {len(val_files)} files to validation directories.")

# Copy test files
for image_path, label_path in test_files:
    shutil.copy(image_path, os.path.join(test_images_dir, os.path.basename(image_path)))
    shutil.copy(label_path, os.path.join(test_labels_dir, os.path.basename(label_path)))
print(f"Copied {len(test_files)} files to testing directories.")

"""## Verify the split

### Subtask:
(Optional) Check the number of files in each directory to ensure the split was done correctly.

**Reasoning**:
Count the number of files in each of the image and label directories for the train, validation, and test sets and print the counts.
"""

import os

# Define paths to the split directories
train_images_dir = os.path.join(base_split_dir, 'train', 'images')
train_labels_dir = os.path.join(base_split_dir, 'train', 'labels')
val_images_dir = os.path.join(base_split_dir, 'val', 'images')
val_labels_dir = os.path.join(base_split_dir, 'val', 'labels')
test_images_dir = os.path.join(base_split_dir, 'test', 'images')
test_labels_dir = os.path.join(base_split_dir, 'test', 'labels')

# Count files in each directory
train_image_count = len(os.listdir(train_images_dir))
train_label_count = len(os.listdir(train_labels_dir))
val_image_count = len(os.listdir(val_images_dir))
val_label_count = len(os.listdir(val_labels_dir))
test_image_count = len(os.listdir(test_images_dir))
test_label_count = len(os.listdir(test_labels_dir))

# Print the counts
print(f"Number of training images: {train_image_count}")
print(f"Number of training labels: {train_label_count}")
print(f"Number of validation images: {val_image_count}")
print(f"Number of validation labels: {val_label_count}")
print(f"Number of testing images: {test_image_count}")
print(f"Number of testing labels: {test_label_count}")

"""## Summary:

### Data Analysis Key Findings

*   The dataset contains 1747 image files, each with a corresponding label file, totaling 1747 image-label pairs.
*   The dataset was split into training, validation, and testing sets with a 70/15/15 ratio, resulting in:
    *   Training set: 1222 image-label pairs.
    *   Validation set: 262 image-label pairs.
    *   Testing set: 263 image-label pairs.
*   Directories were successfully created for each split (train, val, test) with separate subdirectories for images and labels.
*   The image and label files were successfully copied to their respective directories, maintaining the image-label pairing within each set.

### Insights or Next Steps

*   The dataset is now organized into distinct sets ready for training, validation, and testing of a computer vision model.
*   Proceed with loading the data from these split directories for model development and evaluation.

# Task
Build a classification model using a pre-trained MobileNetV2 model, replacing its original classification layer with a new one suitable for classifying images into two categories: "cattle" and "buffaloes".

## Install necessary libraries

### Subtask:
Install TensorFlow for building and training the model.

**Reasoning**:
Install the tensorflow library using pip as requested by the subtask.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install tensorflow

"""## Load mobilenetv2

### Subtask:
Load the pre-trained MobileNetV2 model without the top classification layer.

**Reasoning**:
Load the pre-trained MobileNetV2 model without the top classification layer.
"""

from tensorflow.keras.applications import MobileNetV2

# Load the pre-trained MobileNetV2 model without the top classification layer
base_model = MobileNetV2(weights='imagenet', include_top=False)

"""## Add new layers

### Subtask:
Add a global average pooling layer and a new dense classification layer with 2 units.

**Reasoning**:
Add a global average pooling layer and a new dense classification layer with 2 units to the sequential model.
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense

# Create a Sequential model
model = Sequential()

# Add the base_model to the sequential model
model.add(base_model)

# Add a GlobalAveragePooling2D layer
model.add(GlobalAveragePooling2D())

# Add a new dense classification layer with 2 units and 'softmax' activation
model.add(Dense(2, activation='softmax'))

# Print the model summary to verify the layers
model.summary()

"""## Compile the model

### Subtask:
Compile the modified model with an appropriate optimizer, loss function, and metrics.

**Reasoning**:
Compile the model with the Adam optimizer, categorical crossentropy loss, and accuracy metric.
"""

from tensorflow.keras.optimizers import Adam

# Compile the model
model.compile(optimizer=Adam(),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

print("Model compiled successfully.")

"""## Summary:

### Data Analysis Key Findings

*   The MobileNetV2 model was successfully loaded without its original top classification layer, using pre-trained ImageNet weights.
*   A new sequential model was constructed by adding the MobileNetV2 base model, followed by a `GlobalAveragePooling2D` layer and a `Dense` layer with 2 units and 'softmax' activation for binary classification.
*   The modified model was successfully compiled using the Adam optimizer, 'categorical\_crossentropy' loss function, and 'accuracy' as the evaluation metric.

### Insights or Next Steps

*   The next step is to train the compiled model using a dataset containing images of "cattle" and "buffaloes".
*   After training, evaluate the model's performance on a separate test set to assess its accuracy in classifying the two categories.

# Task
Train a MobileNetV2 model on the "cattle" and "buffaloes" image datasets, using the specified training parameters, and evaluate its performance.

## Set up data generators

### Subtask:
Create data generators for the training, validation, and testing sets to load and preprocess images in batches.

**Reasoning**:
Create data generators for the training, validation, and testing sets using ImageDataGenerator with appropriate parameters and data augmentation for training and validation sets.

# Task
Train a MobileNetV2 model to classify images of cattle and buffaloes, evaluate its performance on a test set, and display the accuracy, precision, recall, and confusion matrix.

## Set up data generators

### Subtask:
Create data generators for the training, validation, and testing sets to load and preprocess images in batches.

**Reasoning**:
Create data generators for the training, validation, and testing sets using ImageDataGenerator with appropriate parameters and data augmentation for training and validation sets.
"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define data generators with data augmentation for training and validation
train_datagen = ImageDataGenerator(
    rescale=1./255, # Normalize pixel values
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

val_datagen = ImageDataGenerator(rescale=1./255) # Only normalize for validation

# No augmentation for test data, just normalization
test_datagen = ImageDataGenerator(rescale=1./255)

# Specify the directories for each split
train_dir = os.path.join(base_split_dir, 'train')
val_dir = os.path.join(base_split_dir, 'val')
test_dir = os.path.join(base_split_dir, 'test')

# Create data generators
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=target_size, # Use the target_size defined earlier
    batch_size=32,
    class_mode='categorical' # Use 'categorical' for one-hot encoding of labels
)

val_generator = val_datagen.flow_from_directory(
    val_dir,
    target_size=target_size,
    batch_size=32,
    class_mode='categorical'
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=target_size,
    batch_size=32,
    class_mode='categorical',
    shuffle=False # Keep data in order for evaluation
)

print("Data generators set up.")

"""## Evaluate the model

### Subtask:
Evaluate the trained model's performance on the test set.

**Reasoning**:
Evaluate the trained model's performance on the test set to assess its generalization ability.
"""

# Evaluate the model on the test set
loss, accuracy = model.evaluate(test_generator)

print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

"""## Calculate and display metrics

### Subtask:
Calculate and display accuracy, precision, recall, and the confusion matrix.

**Reasoning**:
Calculate and display accuracy, precision, recall, and the confusion matrix to provide a comprehensive evaluation of the model's performance.
"""

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# Get predictions from the test generator
test_generator.reset() # Reset the generator to ensure correct order
predictions = model.predict(test_generator)
predicted_classes = np.argmax(predictions, axis=1)

# Get true labels from the test generator
true_classes = test_generator.classes
class_labels = list(test_generator.class_indices.keys())

# Calculate and print classification report
report = classification_report(true_classes, predicted_classes, target_names=class_labels)
print("Classification Report:")
print(report)

# Calculate and print confusion matrix
conf_matrix = confusion_matrix(true_classes, predicted_classes)
print("Confusion Matrix:")
print(conf_matrix)
